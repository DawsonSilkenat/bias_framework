{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 individuals in each group (privileged, unprivileged). 4 of each group are in the positive class, 6 negative.\n",
    "3 from the unprivileged group are classified as positive, 2 are correct. 4 from the privileged group are classified as positive, 3 are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import bias_framework.metrics as metrics\n",
    "\n",
    "df_test_metrics = pd.DataFrame(\n",
    "    {\"Is privileged\" : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}) \n",
    "true_values = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]) \n",
    "pred_values = np.array([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]) \n",
    "\n",
    "privilege_function = lambda x: x[\"Is privileged\"] == 1\n",
    "\n",
    "# df_test_metrics = pd.DataFrame(\n",
    "#     {\"Is Privileged\" : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}) \n",
    "# true_values = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]) \n",
    "# pred_values = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_with_true_class = df_test_metrics.copy()\n",
    "df_dataset_with_true_class[\"Class Label\"] = true_values\n",
    "    \n",
    "df_dataset_with_predicted_class = df_test_metrics.copy()\n",
    "df_dataset_with_predicted_class[\"Class Label\"] = pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is privileged</th>\n",
       "      <th>Class Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Is privileged  Class Label\n",
       "0               0            1\n",
       "1               0            1\n",
       "4               0            1\n",
       "10              1            1\n",
       "11              1            1\n",
       "12              1            1\n",
       "14              1            1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_with_predicted_class[df_dataset_with_predicted_class[\"Class Label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10000000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priv_ratio = len(df_dataset_with_predicted_class[(df_dataset_with_predicted_class[\"Class Label\"] == 1) & (df_dataset_with_predicted_class[\"Is privileged\"] == 1)]) / len(df_dataset_with_predicted_class[(df_dataset_with_predicted_class[\"Is privileged\"] == 1)])\n",
    "\n",
    "unpriv_ratio =  len(df_dataset_with_predicted_class[(df_dataset_with_predicted_class[\"Class Label\"] == 1) & (df_dataset_with_predicted_class[\"Is privileged\"] == 0)]) / len(df_dataset_with_predicted_class[(df_dataset_with_predicted_class[\"Is privileged\"] == 0)])\n",
    "unpriv_ratio\n",
    "\n",
    "unpriv_ratio - priv_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_true_class = StandardDataset(df_dataset_with_true_class, \n",
    "                      label_name=\"Class Label\", \n",
    "                      favorable_classes=[1],\n",
    "                      protected_attribute_names=[\"Is privileged\"], \n",
    "                      privileged_classes=[[1]]\n",
    "                      )\n",
    "# dataset_with_true_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_predicted_class = StandardDataset(df_dataset_with_predicted_class, \n",
    "                      label_name=\"Class Label\", \n",
    "                      favorable_classes=[1],\n",
    "                      protected_attribute_names=[\"Is privileged\"], \n",
    "                      privileged_classes=[[1]]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_metric = ClassificationMetric(dataset_with_true_class, dataset_with_predicted_class, unprivileged_groups=[{\"Is privileged\" : 0}], privileged_groups=[{\"Is privileged\" : 1}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aif360.metrics.classification_metric.ClassificationMetric at 0x32b655ad0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10000000000000003"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metric.statistical_parity_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metric.average_abs_odds_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metric.equal_opportunity_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000009"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metric.error_rate_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daws/.local/share/virtualenvs/bias_framework-lO12qBfZ/lib/python3.11/site-packages/aif360/datasets/standard_dataset.py:122: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[priv, attr] = privileged_values[0]\n",
      "/Users/daws/.local/share/virtualenvs/bias_framework-lO12qBfZ/lib/python3.11/site-packages/aif360/datasets/standard_dataset.py:122: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.0' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[priv, attr] = privileged_values[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fairness': {'statistical parity difference': 0.10000000000000003,\n",
       "  'average odds difference': 0.125,\n",
       "  'equal opportunity difference': 0.25,\n",
       "  'error rate difference': 0.10000000000000009},\n",
       " 'error': {'accuracy': 0.75,\n",
       "  'recall positive class': 0.625,\n",
       "  'recall negative class': 0.8333333333333334,\n",
       "  'recall macro average': 0.7291666666666667,\n",
       "  'precision positive class': 0.7142857142857143,\n",
       "  'precision negative class': 0.7692307692307693,\n",
       "  'precision macro average': 0.7417582417582418,\n",
       "  'f1 score positive class': 0.6666666666666666,\n",
       "  'f1 score negative class': 0.8,\n",
       "  'f1 score macro average': 0.7333333333333334,\n",
       "  'Matthews correlation coefficient': 0.47075654176200415}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.get_all_metrics(df_test_metrics, true_values, pred_values, privilege_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bias_framework-lO12qBfZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
